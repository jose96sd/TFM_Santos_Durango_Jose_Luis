{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0e4547",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "  <div style=\"float: left; width: 50%;\">\n",
    "    <img src=\"https://brandemia.org/sites/default/files/sites/default/files/uoc_nuevo_logo.jpg\" align=\"left\" style=\"width: 80%;\">\n",
    "  </div>\n",
    "  <div style=\"float: right; width: 50%; text-align: right;\">\n",
    "    <h3 style=\"text-align: left; font-weight: bold;\">Optimización del sistema de bicicletas compartidas en la ciudad de Valencia.</h3>\n",
    "    <p style=\"text-align: left; font-weight: bold; font-size: 100%;\">Análisis predictivo, rutas de reparto para el balanceo y gestión eficiente de las estaciones.</p>\n",
    "    <p style=\"margin: 0; text-align: right;\">Jose Luis Santos Durango</p>\n",
    "    <hr style=\"border-top: 1px solid #ccc; margin: 10px 0;\">\n",
    "    <p style=\"margin: 0; padding-top: 22px; text-align:right;\">ETL_Spark.ipynb ·M2.879 · Trabajo Final de Máster · Área 2</p>\n",
    "    <p style=\"margin: 0; text-align:right;\">2023-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "    <p style=\"margin: 0; text-align:right;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "  </div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201e80b",
   "metadata": {},
   "source": [
    "# Procesamiento de los datos históricos con Spark\n",
    "\n",
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f151b17",
   "metadata": {},
   "source": [
    "Lo primero que vamos a hacer en este notebook es configurar el entorno de trabajo de Spark. Para ello, se nos ha asignado un servidor de la UOC con recursos en un cluster de Cloudera, para poder ejecutar mayor cantidad de datos en paralelo, ya que el procesamiento en local se quedaba obsoleto debido a la gran cantidad de datos de los que se dispone.\n",
    "\n",
    "\n",
    "1. [Definición del entorno Spark](#1)\n",
    "        1.1 Descompresión de las carpetas de datos\n",
    "2. [Extracción de los datos históricos de cada estación y almacenamiento en un fichero txt](#2)\n",
    "3. [Conclusiones](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb29638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ceil\n",
    "import os\n",
    "import tarfile\n",
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, avg\n",
    "from datetime import datetime\n",
    "import tarfile\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5857fa3",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# 1. Definición del entorno de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b9ebe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "def start_spark(app_name,master):\n",
    "    \"\"\"\n",
    "    Function to initialize a SparkContext.\n",
    "\n",
    "    Parameters:\n",
    "        app_name (str): Name of the Spark application.\n",
    "        master_mode (str): Spark master URL (e.g., \"local\", \"yarn\", \"spark://host:port\").\n",
    "\n",
    "    Returns:\n",
    "        SparkContext: A SparkContext object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conf = SparkConf().setAppName(app_name).setMaster(master)\n",
    "        sc = SparkContext(conf=conf)\n",
    "        print(\"Spark initialized successfully.\")\n",
    "        return sc\n",
    "    except Exception as e:\n",
    "        print(\"Error initializing Spark:\", str(e))\n",
    "        return None\n",
    "\n",
    "# Set master mode to local\n",
    "sc = start_spark(app_name=\"SparkStart\",master=\"local[1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc2094a",
   "metadata": {},
   "source": [
    "## 1.1 Descompresión las carpetas de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ac43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the compressed files are located\n",
    "directory = \"data\"\n",
    "\n",
    "# Get the list of compressed files\n",
    "compressed_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".tar.gz\")]\n",
    "\n",
    "# Function to decompress a file\n",
    "def extract_file(file):\n",
    "    \"\"\"\n",
    "    Function to extract a tar.gz file.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): Path to the tar.gz file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    output_folder = \"data_descomp\"  # Output folder for extracted files\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Create the output folder if it doesn't exist\n",
    "    with tarfile.open(file, 'r:gz') as tar_ref:\n",
    "        tar_ref.extractall(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbe91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress files in parallel\n",
    "pool = ThreadPool(len(compressed_files))\n",
    "pool.map(extract_file, compressed_files)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42034d21",
   "metadata": {},
   "source": [
    "Una vez que tenemos los datos descomprimidos en la carpeta data_descomp de nuestro directorio, lo que vamos a hacer es procesar los datos con Spark. En el notebook ETL_Python para procesar los datos y dejarlos en granularidad horaria, que será la granularidad con la que vamos a trabajar, lo que hicimos fue recorrer cada 60 líneas dentro de los archivos de datos de las estaciones, tomando como referencia un único registro por hora. Debido a problemas de calidad de los datos históricos, no todas las horas coincidían en el minuto, por lo que imputamos el minuto 00 para todos los registros independientemente de que no correspondiera al minuto 00. En este notebook, vamos a procesar los datos tomando como referencia el valor medio de cada hora, obteniendo así un valor más representativo para cada hora. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bdd2e",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# 2. Extracción de los datos históricos de cada estación y almacenamiento en un fichero txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5017f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(data):\n",
    "    \"\"\"\n",
    "    Get directories containing data for specified years.\n",
    "\n",
    "    Parameters:\n",
    "        data_years (list): A list of integers representing the years.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of directory paths containing data for the specified years.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_directories = []\n",
    "    data_dir_test = os.path.join(\"data_descomp\", data)\n",
    "    for month in os.listdir(data_dir_test):\n",
    "        if month != \".DS_Store\":\n",
    "            data_dir_month = os.path.join(data_dir_test, month)\n",
    "            for day in os.listdir(data_dir_month):\n",
    "                if day != \".DS_Store\":\n",
    "                    start_directories.append(os.path.join(data_dir_month, day))\n",
    "    return start_directories\n",
    "\n",
    "def process_data(data_years):\n",
    "    \"\"\"\n",
    "    Process data from directories using Spark.\n",
    "\n",
    "    Parameters:\n",
    "        data_years (list): A list of integers representing the years.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataProcessing\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    start_directories = get_directories(data_years)\n",
    "    output = 'data_descomp/data_avg.txt'\n",
    "    \n",
    "    # Define a function to process each line and calculate the average for each station, day, and hour\n",
    "    def process_line(line, station_id):\n",
    "        data = line.strip().split(',')\n",
    "        timestamp_str = data[0]\n",
    "        bikes = float(data[1])\n",
    "        parking = float(data[2])\n",
    "        # Convertir la cadena de timestamp a un objeto datetime\n",
    "        timestamp = datetime.strptime(timestamp_str, '%Y/%m/%d %H:%M:%S')\n",
    "        return (station_id, timestamp, bikes, parking)\n",
    "    \n",
    "    # Read data from directories into an RDD and process each line\n",
    "    rdd = spark.sparkContext.parallelize(start_directories) \\\n",
    "        .flatMap(lambda directory: [(os.path.basename(directory), os.path.join(directory, file)) for file in os.listdir(directory)]) \\\n",
    "        .filter(lambda station: 'checkpoints' not in station[1]) \\\n",
    "        .flatMap(lambda station: [(os.path.basename(station[1]), line) for line in open(station[1], 'r', encoding='latin-1').readlines()])\n",
    "    \n",
    "    # Process each line\n",
    "    rdd = rdd.map(lambda x: process_line(x[1], x[0]))\n",
    "    \n",
    "    # Convert RDD to DataFrame\n",
    "    df = spark.createDataFrame(rdd, ['station_id', 'timestamp', 'bikes', 'parking'])\n",
    "    \n",
    "    # Extract date and hour from timestamp\n",
    "    df = df.withColumn('date', date_format('timestamp', 'yyyy-MM-dd')) \\\n",
    "           .withColumn('hour', date_format('timestamp', 'HH'))\n",
    "    \n",
    "    # Calculate average values per station, day, and hour\n",
    "    avg_df = df.groupBy(\"station_id\", \"date\", \"hour\").agg(\n",
    "        avg(\"bikes\").alias(\"avg_bikes\"),\n",
    "        avg(\"parking\").alias(\"avg_parking\")\n",
    "    )\n",
    "    \n",
    "    # Write results to text file without header\n",
    "    avg_df.write.mode(\"overwrite\").csv(output, header=False)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "process_data('2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454da9fa",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# 3. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb100f1",
   "metadata": {},
   "source": [
    "Una vez hemos procesado los datos con Spark, se debería haber creado un fichero data_avg.txt en el sistema de ficheros distribuidos de Hadoop, de forma que con el comando -getmerge podamos unir en un único fichero local que será el que usaremos para el tratamiento de nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cce4cd",
   "metadata": {},
   "source": [
    "Veamos que efectivamente existe el fichero en el sistema distribuido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72bd60eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxr-xr-x   - josant05 josant05          0 2024-04-17 00:56 data_descomp/2021\n",
      "drwxr-xr-x   - josant05 josant05          0 2024-04-17 09:09 data_descomp/2022\n",
      "drwxr-xr-x   - josant05 josant05          0 2024-04-17 00:51 data_descomp/2023\n",
      "drwxr-xr-x   - josant05 josant05          0 2024-04-22 23:16 data_descomp/data_avg.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data_descomp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3383a4",
   "metadata": {},
   "source": [
    "Veamos el dataset una vez hemos cargado los datos en un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c34a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2416382 entries, 0 to 2416381\n",
      "Data columns (total 5 columns):\n",
      "station_id      int64\n",
      "date            object\n",
      "hour            object\n",
      "bikes_avg       float64\n",
      "parkings_avg    float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 92.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>bikes_avg</th>\n",
       "      <th>parkings_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>2021-01-21</td>\n",
       "      <td>03:00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>234</td>\n",
       "      <td>2021-01-21</td>\n",
       "      <td>09:00</td>\n",
       "      <td>4.483333</td>\n",
       "      <td>10.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>2021-01-21</td>\n",
       "      <td>10:00</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>9.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>2021-01-21</td>\n",
       "      <td>17:00</td>\n",
       "      <td>21.716667</td>\n",
       "      <td>2.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>236</td>\n",
       "      <td>2021-01-21</td>\n",
       "      <td>10:00</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>12.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id        date   hour  bikes_avg  parkings_avg\n",
       "0          48  2021-01-21  03:00  10.000000     10.000000\n",
       "1         234  2021-01-21  09:00   4.483333     10.516667\n",
       "2         103  2021-01-21  10:00   5.833333      9.166667\n",
       "3          18  2021-01-21  17:00  21.716667      2.283333\n",
       "4         236  2021-01-21  10:00   2.250000     12.750000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a dataframe with a header\n",
    "df = pd.read_csv('data_avg.txt', header=None, names=['station_id', 'date', 'hour', 'bikes_avg', 'parkings_avg'])\n",
    "\n",
    "# Convert column 'hour' to the format 'h:mm'\n",
    "df['hour'] = pd.to_datetime(df['hour'], format='%H').dt.strftime('%H:%M')\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
