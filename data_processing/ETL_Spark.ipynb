{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a572eab3",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "  <div style=\"float: left; width: 50%;\">\n",
    "    <img src=\"https://brandemia.org/sites/default/files/sites/default/files/uoc_nuevo_logo.jpg\" align=\"left\" style=\"width: 80%;\">\n",
    "  </div>\n",
    "  <div style=\"float: right; width: 50%; text-align: right;\">\n",
    "    <h3 style=\"text-align: left; font-weight: bold;\">Optimización del sistema de bicicletas compartidas en la ciudad de Valencia.</h3>\n",
    "    <p style=\"text-align: left; font-weight: bold; font-size: 100%;\">Análisis predictivo, rutas de reparto para el balanceo y gestión eficiente de las estaciones.</p>\n",
    "    <p style=\"margin: 0; text-align: right;\">Jose Luis Santos Durango</p>\n",
    "    <hr style=\"border-top: 1px solid #ccc; margin: 10px 0;\">\n",
    "    <p style=\"margin: 0; padding-top: 22px; text-align:right;\">ETL_Spark.ipynb ·M2.879 · Trabajo Final de Máster · Área 2</p>\n",
    "    <p style=\"margin: 0; text-align:right;\">2023-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "    <p style=\"margin: 0; text-align:right;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "  </div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b58927",
   "metadata": {},
   "source": [
    "# Procesamiento de los datos históricos con Spark\n",
    "\n",
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2fb73",
   "metadata": {},
   "source": [
    "Lo primero que vamos a hacer en este notebook es configurar el entorno de trabajo de Spark. Para ello, se nos ha asignado un servidor de la UOC con recursos en un cluster de Cloudera, para poder ejecutar mayor cantidad de datos en paralelo, ya que el procesamiento en local se quedaba obsoleto debido a la gran cantidad de datos de los que se dispone.\n",
    "\n",
    "\n",
    "1. [Definición del entorno Spark](#1)\n",
    "        1.1 Descompresión de las carpetas de datos\n",
    "2. [Extracción de los datos históricos de cada estación y almacenamiento en un fichero txt](#2)\n",
    "3. [Conclusiones](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f307e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ceil\n",
    "import os\n",
    "import tarfile\n",
    "import time\n",
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, date_format, avg, max, lag, sum as spark_sum, expr\n",
    "from datetime import datetime\n",
    "import tarfile\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38df85",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# 1. Definición del entorno de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55514d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "def start_spark(app_name,master):\n",
    "    \"\"\"\n",
    "    Function to initialize a SparkContext.\n",
    "\n",
    "    Parameters:\n",
    "        app_name (str): Name of the Spark application.\n",
    "        master_mode (str): Spark master URL (e.g., \"local\", \"yarn\", \"spark://host:port\").\n",
    "\n",
    "    Returns:\n",
    "        SparkContext: A SparkContext object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conf = SparkConf().setAppName(app_name).setMaster(master)\n",
    "        sc = SparkContext(conf=conf)\n",
    "        print(\"Spark initialized successfully.\")\n",
    "        return sc\n",
    "    except Exception as e:\n",
    "        print(\"Error initializing Spark:\", str(e))\n",
    "        return None\n",
    "\n",
    "# Set master mode to local\n",
    "sc = start_spark(app_name=\"SparkStart\",master=\"local[1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f17cc",
   "metadata": {},
   "source": [
    "## 1.1 Descompresión las carpetas de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea9a6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the compressed files are located\n",
    "directory = \"data\"\n",
    "\n",
    "# Get the list of compressed files\n",
    "compressed_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".tar.gz\")]\n",
    "\n",
    "# Function to decompress a file\n",
    "def extract_file(file):\n",
    "    \"\"\"\n",
    "    Function to extract a tar.gz file.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): Path to the tar.gz file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    output_folder = \"data_descomp\"  # Output folder for extracted files\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Create the output folder if it doesn't exist\n",
    "    with tarfile.open(file, 'r:gz') as tar_ref:\n",
    "        tar_ref.extractall(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5bfe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress files in parallel\n",
    "#pool = ThreadPool(len(compressed_files))\n",
    "#pool.map(extract_file, compressed_files)\n",
    "#pool.close()\n",
    "#pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbc9d1",
   "metadata": {},
   "source": [
    "Una vez que tenemos los datos descomprimidos en la carpeta data_descomp de nuestro directorio, lo que vamos a hacer es procesar los datos con Spark. En el notebook ETL_Python para procesar los datos y dejarlos en granularidad horaria, que será la granularidad con la que vamos a trabajar, lo que hicimos fue recorrer cada 60 líneas dentro de los archivos de datos de las estaciones, tomando como referencia un único registro por hora. Debido a problemas de calidad de los datos históricos, no todas las horas coincidían en el minuto, por lo que imputamos el minuto 00 para todos los registros independientemente de que no correspondiera al minuto 00. En este notebook, vamos a procesar los datos tomando como referencia el valor medio de cada hora, obteniendo así un valor más representativo para cada hora. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5120cabc",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# 2. Extracción de los datos históricos de cada estación y almacenamiento en un fichero txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0a281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(data):\n",
    "    \"\"\"\n",
    "    Get directories containing data for specified years.\n",
    "\n",
    "    Parameters:\n",
    "        data_years (list): A list of integers representing the years.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of directory paths containing data for the specified years.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_directories = []\n",
    "    data_dir_test = os.path.join(\"data_descomp\", data)\n",
    "    for month in os.listdir(data_dir_test):\n",
    "        if month != \".DS_Store\":\n",
    "            data_dir_month = os.path.join(data_dir_test, month)\n",
    "            for day in os.listdir(data_dir_month):\n",
    "                if day != \".DS_Store\":\n",
    "                    start_directories.append(os.path.join(data_dir_month, day))\n",
    "    return start_directories\n",
    "\n",
    "def process_data(data_years):\n",
    "    \"\"\"\n",
    "    Process data from directories using Spark.\n",
    "\n",
    "    Parameters:\n",
    "        data_years (list): A list of integers representing the years.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataProcessing\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    start_directories = get_directories(data_years)\n",
    "    output = 'data_descomp/data_trips.txt'\n",
    "    \n",
    "\n",
    "    def process_line(line, station_id):\n",
    "        data = line.strip().split(',')\n",
    "        timestamp_str = data[0]\n",
    "        bikes = float(data[1])\n",
    "        parking = float(data[2])\n",
    "        # Convertir la cadena de timestamp a un objeto datetime\n",
    "        timestamp = datetime.strptime(timestamp_str, '%Y/%m/%d %H:%M:%S')\n",
    "        return (station_id, timestamp, bikes, parking)\n",
    "\n",
    "    # Read data from directories into an RDD and process each line\n",
    "    rdd = spark.sparkContext.parallelize(start_directories) \\\n",
    "            .flatMap(lambda directory: [(os.path.basename(directory), os.path.join(directory, file)) for file in os.listdir(directory)]) \\\n",
    "            .filter(lambda station: 'checkpoints' not in station[1]) \\\n",
    "            .flatMap(lambda station: [(os.path.basename(station[1]), line) for line in open(station[1], 'r', encoding='latin-1').readlines()])\n",
    "\n",
    "    # Process each line and calculate total_trips\n",
    "    rdd = rdd.map(lambda x: process_line(x[1], x[0])) \\\n",
    "         .toDF(['station_id', 'timestamp', 'bikes', 'parking']) \\\n",
    "         .withColumn('date', date_format('timestamp', 'yyyy-MM-dd')) \\\n",
    "         .withColumn('hour', date_format('timestamp', 'HH')) \\\n",
    "         .withColumn(\"bikes_diff\", expr(\"abs(bikes - lag(bikes, 1) over (partition by station_id, date, hour order by timestamp))\")) \\\n",
    "         .fillna({\"bikes_diff\": 0}) \\\n",
    "         .withColumn(\"total_trips\", spark_sum(\"bikes_diff\").over(Window.partitionBy(\"station_id\", \"date\", \"hour\").orderBy(\"timestamp\")))\n",
    "\n",
    "    # Calculate average values per station, day, and hour\n",
    "    avg_df = rdd.groupBy(\"station_id\", \"date\", \"hour\").agg(\n",
    "            avg(\"bikes\").alias(\"avg_bikes\"),\n",
    "            avg(\"parking\").alias(\"avg_parking\"),\n",
    "            max(\"total_trips\").alias(\"max_total_trips\") # adding max value for the hour as it is a counter\n",
    "        )\n",
    "        \n",
    "    # Write results to text file without header\n",
    "    avg_df.write.mode(\"overwrite\").csv(output, header=False)\n",
    "\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c57c7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's execute data_processor for 2021 data\n",
    "#process_data('2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fc6be",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# 3. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55b784",
   "metadata": {},
   "source": [
    "Una vez hemos procesado los datos con Spark, se debería haber creado un fichero data_trips.txt en el sistema de ficheros distribuidos de Hadoop, de forma que con el comando -getmerge podamos unir en un único fichero local que será el que usaremos para el tratamiento de nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab3a5b",
   "metadata": {},
   "source": [
    "Veamos que efectivamente existe el fichero en el sistema distribuido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70301721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "drwxr-xr-x   - josant05 josant05          0 2024-04-17 00:56 data_descomp/2021\n",
      "drwxr-xr-x   - josant05 josant05          0 2024-04-17 09:09 data_descomp/2022\n",
      "drwxr-xr-x   - josant05 josant05          0 2024-04-17 00:51 data_descomp/2023\n",
      "drwxr-xr-x   - josant05 josant05          0 2024-04-22 23:16 data_descomp/data_avg.txt\n",
      "drwxr-xr-x   - josant05 josant05          0 2024-04-26 02:08 data_descomp/data_trips.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data_descomp\n",
    "!hdfs dfs -getmerge data_descomp/data_trips.txt data_trips.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c45274",
   "metadata": {},
   "source": [
    "Veamos el dataset una vez hemos cargado los datos en un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0228525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2416382 entries, 0 to 2416381\n",
      "Data columns (total 6 columns):\n",
      "station_id      int64\n",
      "date            object\n",
      "hour            object\n",
      "bikes_avg       float64\n",
      "parkings_avg    float64\n",
      "total_trips     float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 110.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>bikes_avg</th>\n",
       "      <th>parkings_avg</th>\n",
       "      <th>total_trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>918104</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687860</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>01:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235365</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>02:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136048</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>03:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473982</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>04:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389512</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>05:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244836</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>06:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410144</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>07:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546429</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>08:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494601</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>09:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96816</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>10:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627505</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>11:00</td>\n",
       "      <td>13.533333</td>\n",
       "      <td>11.466667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96817</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>12:00</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558461</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>13:00</td>\n",
       "      <td>13.066667</td>\n",
       "      <td>11.933333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764235</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>14:00</td>\n",
       "      <td>13.433333</td>\n",
       "      <td>11.566667</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277322</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>15:00</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114641</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>16:00</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510104</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>17:00</td>\n",
       "      <td>13.716667</td>\n",
       "      <td>11.283333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316640</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>18:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304759</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>19:00</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253212</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>20:00</td>\n",
       "      <td>15.516667</td>\n",
       "      <td>9.483333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651643</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>21:00</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966343</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>22:00</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12017</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>23:00</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         station_id        date   hour  bikes_avg  parkings_avg  total_trips\n",
       "918104            1  2021-01-01  00:00  14.333333     10.666667          1.0\n",
       "687860            1  2021-01-01  01:00  14.000000     11.000000          0.0\n",
       "2235365           1  2021-01-01  02:00  14.000000     11.000000          0.0\n",
       "1136048           1  2021-01-01  03:00  14.000000     11.000000          0.0\n",
       "1473982           1  2021-01-01  04:00  14.000000     11.000000          0.0\n",
       "1389512           1  2021-01-01  05:00  14.000000     11.000000          0.0\n",
       "1244836           1  2021-01-01  06:00  14.000000     11.000000          0.0\n",
       "410144            1  2021-01-01  07:00  14.000000     11.000000          0.0\n",
       "1546429           1  2021-01-01  08:00  14.000000     11.000000          0.0\n",
       "494601            1  2021-01-01  09:00  14.000000     11.000000          0.0\n",
       "96816             1  2021-01-01  10:00  14.000000     11.000000          0.0\n",
       "627505            1  2021-01-01  11:00  13.533333     11.466667          1.0\n",
       "96817             1  2021-01-01  12:00  13.000000     12.000000          0.0\n",
       "1558461           1  2021-01-01  13:00  13.066667     11.933333          1.0\n",
       "1764235           1  2021-01-01  14:00  13.433333     11.566667          3.0\n",
       "277322            1  2021-01-01  15:00  13.000000     12.000000          0.0\n",
       "2114641           1  2021-01-01  16:00  13.000000     12.000000          0.0\n",
       "1510104           1  2021-01-01  17:00  13.716667     11.283333          1.0\n",
       "1316640           1  2021-01-01  18:00  14.000000     11.000000          0.0\n",
       "1304759           1  2021-01-01  19:00  15.100000      9.900000          2.0\n",
       "253212            1  2021-01-01  20:00  15.516667      9.483333          1.0\n",
       "651643            1  2021-01-01  21:00  14.750000     10.250000          1.0\n",
       "966343            1  2021-01-01  22:00  13.800000     11.200000          2.0\n",
       "12017             1  2021-01-01  23:00  14.000000     11.000000          0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a dataframe with a header\n",
    "df = pd.read_csv('data_trips.txt', header=None, names=['station_id', 'date', 'hour', 'bikes_avg', 'parkings_avg', 'total_trips'])\n",
    "\n",
    "# Convert column 'hour' to the format 'hh:mm'\n",
    "df['hour'] = pd.to_datetime(df['hour'], format='%H').dt.strftime('%H:%M')\n",
    "\n",
    "# Let's get some ordered data and check manually if total_trips is correct\n",
    "df_sorted = df.sort_values(by=['date', 'station_id', 'hour'], ascending=True)\n",
    "\n",
    "df.info()\n",
    "df_sorted.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85bec7",
   "metadata": {},
   "source": [
    "Podemos ver que efectivamente se han calculado los valores de los viajes por hora de forma correcta. Hemos comprobado el primer día para la primera estación de forma manual y los valores coinciden. Respecto al dataset, podemos ver que el total de registros es 2.416.382 registros, lo cual nos dice que hay algunos registros olvidados por algún problema de calidad de los datos. Se comprobará en la parte del análisis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
