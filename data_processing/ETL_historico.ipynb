{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d0df734",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "  <div style=\"float: left; width: 50%;\">\n",
    "    <img src=\"https://brandemia.org/sites/default/files/sites/default/files/uoc_nuevo_logo.jpg\" align=\"left\" style=\"width: 80%;\">\n",
    "  </div>\n",
    "  <div style=\"float: right; width: 50%; text-align: right;\">\n",
    "    <h3 style=\"text-align: left; font-weight: bold;\">Optimización del sistema de bicicletas compartidas en la ciudad de Valencia.</h3>\n",
    "    <p style=\"text-align: left; font-weight: bold; font-size: 100%;\">Análisis predictivo, rutas de reparto para el balanceo y gestión eficiente de las estaciones.</p>\n",
    "    <p style=\"margin: 0; text-align: right;\">Jose Luis Santos Durango</p>\n",
    "    <hr style=\"border-top: 1px solid #ccc; margin: 10px 0;\">\n",
    "    <p style=\"margin: 0; padding-top: 22px; text-align:right;\">ETL_historico.ipynb · M2.879 · Trabajo Final de Máster · Área 2</p>\n",
    "    <p style=\"margin: 0; text-align:right;\">2023-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "    <p style=\"margin: 0; text-align:right;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "  </div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd3a946",
   "metadata": {},
   "source": [
    "# Preparación de los datos históricos de Valenbisi\n",
    "\n",
    "En este notebook vamos a realizar el tratamiento de los datos históricos de Valenbisi. Los datos que tenemos, como ya se ha mencionado en el Notebook 00. ETL_other_data.ipynb, son de 4 años (2020-2023) con una granularidad de minuto (580 millones de registros aprox.) Para lograr este objetivo, vamos a descomprimir los datos de las carpetas que tenemos. Después les daremos el fomato deseado y crearemos un dataframe para poder guardarlo en un fichero con todos los datos formateados y compactados en la misma estructura.\n",
    "\n",
    "\n",
    "1. [Descompresión de los datos históricos](#1)\n",
    "        1.1 Extracción de los datos históricos de cada estación y almacenamiento en un fichero txt\n",
    "2. [Creación de un dataframe con los datos históricos](#2)\n",
    "3. [Conclusiones](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861df896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ceil\n",
    "import os\n",
    "import tarfile\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b371896",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Descompresión de los datos históricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35c8eb",
   "metadata": {},
   "source": [
    "Los datos históricos han sido proporcionados en 4 carpetas comprimidas, cada una con el nombre del año de los datos. En nuestro caso, solo vamos a considerar datos de 2021, como ya se ha comentado. La estructura que siguen estos ficheros es la siguiente:\n",
    "\n",
    "- Año\n",
    "    - Mes\n",
    "        - Día\n",
    "            - Estaciones (numeradas de 1 a 276) en formato txt. Cada fichero txt tiene la siguiente estructura:\n",
    "                - Fecha y hora\n",
    "                - Bicicletas disponibles\n",
    "                - Parking disponibles\n",
    "\n",
    "Para descomprimir los datos vamos a crear una función que se ejecutará sobre cada uno de los años de la lista que le pasemos, en nuestro caso 2021. Es importante que para poder ejecutar esta función, se hayan guardado previamente los datos en una carpeta llamada data, en el escritorio local de la persona que ejecuta el sript. Esta función generará como resultado otra carpeta llamada data_descomp que contendrá los datos descomprimidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e40226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_files_tar_gz(data_years):\n",
    "    \"\"\"\n",
    "    Extract files tar.gz for years in data_years list.\n",
    "\n",
    "    Parameters:\n",
    "        data_years (list): A list of ints with the years folders to extract\n",
    "\n",
    "    Raises:\n",
    "        OSError: if OS is not supported\n",
    "\n",
    "    \"\"\"\n",
    "    # Get the path for desktop folder\n",
    "    if os.name == 'nt':  # Windows\n",
    "        desktop_path = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop')\n",
    "    elif os.name == 'posix':  # macOS y Linux\n",
    "        desktop_path = os.path.join(os.path.join(os.path.expanduser('~')), 'Desktop')\n",
    "    else:\n",
    "        raise OSError(\"Non supported operative system\")\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for year in data_years:\n",
    "        # Important to store the data folder in desktop\n",
    "        folder_name = os.path.join(desktop_path,'data/') + str(year) + '.tar.gz'\n",
    "        \n",
    "        # Directorio de destino para la extracción\n",
    "        destination_directory = os.path.join(desktop_path, 'data_descomp')\n",
    "\n",
    "        # Abrir el archivo tar.gz\n",
    "        with tarfile.open(folder_name, \"r:gz\") as file:\n",
    "            # Extraer todo el contenido en el directorio de destino\n",
    "            file.extractall(destination_directory)\n",
    "        print(f\"The folder {folder_name} has been descompressed in {destination_directory}\")\n",
    "\n",
    "    # Calcular el tiempo de ejecución\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c81bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder /Users/jose/Desktop/data/2021.tar.gz has been descompressed in /Users/jose/Desktop/data_descomp\n",
      "Execution time: 39.50362515449524 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execution\n",
    "years = [2021]\n",
    "\n",
    "# Descompressed data\n",
    "extract_files_tar_gz(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a90ec0",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Extracción de los datos históricos de cada estación y almacenamiento en un fichero txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92abb365",
   "metadata": {},
   "source": [
    "Una vez los datos han sido descomprimidos en el directorio data_descomp guardado en el escritorio vamos a realizar una transformación sobre los mismos para poder obtener una base de datos estándar sobre la que trabajar. La principal transformación que vamos a realizar será centralizar todos los registros separados en ficheros por estaciones en la jerarquía de carpetas Año/Mes/Día/Estación, y añadir la información de la estación a cada registro, para así obtener un fichero en formato texto con las siguientes columnas separadas por comas: ID de la estación, fecha y hora, bicicletas disponibles, bornetas disponibles.\n",
    "\n",
    "Para abordar esta tarea usaremos dos funciones. Una función nos ayudará a listar los directorios de todos los ficheros, es decir las rutas donde se localizan los archivos con los datos de las estaciones. La segunda función, para cada directorio de fichero con datos, cogerá esos datos y a cada registro le añadirá la información de la estación, volcando después toda la información en un archivo de texto en el directorio principal llamado datos.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd266003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(data_years):\n",
    "    \"\"\"\n",
    "    Get directories containing data for specified years.\n",
    "\n",
    "    Parameters:\n",
    "        data_years (list): A list of integers representing the years.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of directory paths containing data for the specified years.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If the operating system is not supported.\n",
    "    \"\"\"\n",
    "    if os.name == 'nt':  # Windows\n",
    "        data_dir = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop') + '/data_descomp'\n",
    "    elif os.name == 'posix':  # macOS and Linux\n",
    "        data_dir = os.path.join(os.path.join(os.path.expanduser('~')), 'Desktop') + '/data_descomp'\n",
    "    else:\n",
    "        raise OSError(\"Operating system not supported\")\n",
    "        \n",
    "    start_directories = []\n",
    "    for year in data_years:\n",
    "        data_dir_year = os.path.join(data_dir, str(year))\n",
    "        for month in os.listdir(data_dir_year):\n",
    "            if month != \".DS_Store\":\n",
    "                data_dir_month = os.path.join(data_dir_year, month)\n",
    "                for day in os.listdir(data_dir_month):\n",
    "                    if day != \".DS_Store\":\n",
    "                        start_directories.append(os.path.join(data_dir_month, day))\n",
    "    return start_directories\n",
    "\n",
    "def data_to_txt(data_years):\n",
    "    \"\"\"\n",
    "    Write data from directories to a text file.\n",
    "\n",
    "    Parameters:\n",
    "        data_years (list): A list of integers representing the years.\n",
    "\n",
    "    \"\"\"\n",
    "    start_directories = get_directories(data_years)\n",
    "    output = '/Users/jose/Desktop/data_descomp/datos.txt'  # Path where the text file will be saved\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(output, 'w') as txt_file:\n",
    "        for directory in start_directories:\n",
    "            for station_id in os.listdir(directory):\n",
    "                station_path = os.path.join(directory, station_id)\n",
    "                with open(station_path, 'r', encoding='latin-1') as file:\n",
    "                    lines = file.readlines()\n",
    "                    total_lines = len(lines)\n",
    "                    for i in range(0, total_lines, 60):\n",
    "                        data = lines[i].strip().split(',')\n",
    "                        data_station_id = [station_id] + data\n",
    "                        txt_file.write(','.join(data_station_id) + '\\n')\n",
    "    \n",
    "    # Calculate the execution time\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9daf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 52.79212808609009 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execution\n",
    "years = [2021]\n",
    "data_to_txt(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97431ce2",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Creación de un dataframe con los datos históricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9d45af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   station_id            timestamp  bikes  parking        date   hour\n",
      "0         135  2021/03/03 00:00:00     16        4  03/03/2021  00:00\n",
      "1         135  2021/03/03 01:00:00     16        4  03/03/2021  01:00\n",
      "2         135  2021/03/03 02:00:00     16        4  03/03/2021  02:00\n",
      "3         135  2021/03/03 03:00:00     16        4  03/03/2021  03:00\n",
      "4         135  2021/03/03 04:00:00     16        4  03/03/2021  04:00\n",
      "Execution time: 35.27678203582764 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load the TXT file into a DataFrame\n",
    "output = '/Users/jose/Desktop/data_descomp/datos.txt'\n",
    "df = pd.read_csv(output, header=None, names=[\"station_id\", \"timestamp\", \"bikes\", \"parking\"])\n",
    "\n",
    "# Split the timestamp column into separate date and hour columns\n",
    "df[['date', 'hour']] = df['timestamp'].str.split(' ', expand=True)\n",
    "\n",
    "# Convert the 'date' column to dd/mm/yyyy format\n",
    "df['date'] = pd.to_datetime(df['date']).dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Convert the 'hour' column to h:mm format\n",
    "df['hour'] = pd.to_datetime(df['hour']).dt.strftime('%H:%M')\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "print(df.head())\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36500eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2417213 entries, 0 to 2417212\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Dtype \n",
      "---  ------      ----- \n",
      " 0   station_id  int64 \n",
      " 1   timestamp   object\n",
      " 2   bikes       int64 \n",
      " 3   parking     int64 \n",
      " 4   date        object\n",
      " 5   hour        object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 110.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97484ba3",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a9efdd",
   "metadata": {},
   "source": [
    "Tras realizar varios intentos de optimización del código, al tratarse de una gran cantidad de registros es imposible tratar los datos con un notebook de Python ejecutando con los recursos de la CPU local. Por este motivo se decide escalar con el tutor la incidencia al departamento de Big Data para que nos proporcionen un servidor donde podamos ejecutar Spark y guardar la información en ficheros del sistema distribuido de Hadoop. Mientras tanto, la solución que hemos planteado en este Notebook, será procesar los datos cada 60 líneas, tomando así como referencia el valor de la estación cada hora. La idea era procesar los valores medios para cada hora. también podemos comparar con estos datos y calcular así un nuevo parámetro: la desviación estándar de cada estación cada hora de referencia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
